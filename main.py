# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_AY0HtKd2_TNQXjYPA_s17bTWvxyo-gb
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# %matplotlib inline

raw_df = pd.read_csv('data/SE_rents2018_train.csv', index_col=0)

raw_test_df = pd.read_csv('data/SE_rents2018_test1.csv', index_col=0)

"""# Data Summarize"""

raw_df.describe()

raw_df.shape

raw_df.info()

raw_df['rent'].hist(bins=100)

"""### Seperate all features into continuous, categorical and binary features.

For those none relatived features, we have excluded them from the features grouping: 
- addr_unit: no relationship
- building_id: no relationship
- addr_city: hard to encode
- addr_zip: hard to encode
- addr_street: hard to process
- neighborhood: hard to encode
- line: hard to encode
- bin: no relationship
- bbl: no relationshio
- description: hard to build a NLP model
- unit: no relationship
"""

continuous_features =['bathrooms','bedrooms','size_sqft','floor_count','year_built','min_to_subway','floornumber', 'addr_lat', 'addr_lon']
categorical_features =['borough']
binary_features = ['has_doorman', 'has_elevator', 'has_fireplace', 'has_dishwasher','is_furnished', 'has_gym', 'allows_pets', 
                   'has_washer_dryer','has_garage', 'has_roofdeck', 'has_concierge', 'has_pool', 'has_garden',
                   'has_childrens_playroom', 'no_fee', ]

unique_count = [] 
for feature in raw_df.columns:
  unique_count.append(raw_df[feature].nunique())
count_df = pd.DataFrame({'Feature':raw_df.columns,'unique count': unique_count})
count_df

"""### use pair coorelation for continuous features"""

continuous_df = raw_df[continuous_features+['rent']]
continuous_df.corr()['rent'][:-1]

"""### Check coorelation for binary features"""

raw_df[binary_features+['rent']].corr()['rent'][:-1]
coor_results= []

for feature in binary_features:
  df = raw_df.groupby([feature]).aggregate(['mean'])['rent']
  df[feature]= df.index
  coor_results.append(df.corr().iloc[0][1])
coor_df = pd.DataFrame({'Coorelation': coor_results,'Feature':binary_features})
coor_df

"""As we can see in the correlation table, all binrary features highly affected the rents. When we build the models, we should include all binary features.

### Check coorelation for categorical features
Need to do the binary first, then check the coorelation for categorical features, should be doen by group two
"""

"""# Data Cleaning

### Handling missing data
In order to handle missing data in this dataset, we frist find and count all the null values.
"""

raw_df.isna().sum()

"""As we can see from the result,there are missing data appearing on addr_unit, bin, year_built, min_to_subway, description, neighborhood, unit, floornumber, and line. 

Since we already found out that addr_unit and bin has no relationship to the value of rent, and it is hard to build a NLP model around description in the other hand, so we don't really need to worry about these data that has not much impact to our final result.

For all other related cases, since we have a dataset with 12000 rows in total and it is hard to find the replaceable values for them, thus we can afford to drop them in order to keep the modeling process clean.
"""

# we will be dropping the rows which we don't have values for year_built, min_to_subway, neighborhood, and floornumber.
# and we will call the new df md_df

md_df = raw_df.loc[
    raw_df.year_built.notnull() &
    raw_df.min_to_subway.notnull() & 
    raw_df.neighborhood.notnull() & 
    raw_df.floornumber.notnull()
]

# use mode to replace NAN value, compare both method when creating models
# md_df = raw_df.loc[
#     raw_df.year_built.notnull() &
#     raw_df.min_to_subway.notnull() & 
#     raw_df.neighborhood.notnull() & 
# ]

# md_df['floornumber'].fillna(md_df['floornumber'].mode()[0], inplace=True)

print("original shape of dataset:",raw_df.shape)
print("shape of dataset after handling missing data:",md_df.shape)

"""## remove outliers"""

for feature in continuous_features:
    md_df.plot.scatter(feature, 'rent')

md_df.loc[md_df['size_sqft']==0].shape

"""## drop size_sqrt = 0 for now, since there are 713 rows, might replace with mode when creating models"""

def remove_outliers(md_df, feature, low_value, high_value):
    print(feature, ': ', md_df.shape)
    md_df = md_df[md_df[feature]>low_value]
    md_df = md_df[md_df[feature]<=high_value]
    md_df.reset_index(drop=True,inplace=True)
    print(feature, ': ', md_df.shape)
    return md_df

md_df = remove_outliers(md_df, 'rent', 0, 30000)
md_df = remove_outliers(md_df, 'bathrooms', 0, 12)
md_df = remove_outliers(md_df, 'size_sqft', 0, 10000)
md_df = remove_outliers(md_df, 'floor_count', 0, 80)
md_df = remove_outliers(md_df, 'year_built', 1700, 2019)
md_df = remove_outliers(md_df, 'min_to_subway', 0, 60)
md_df = remove_outliers(md_df, 'floornumber', 0, 60)

md_df['year_built'] = 2019 - md_df['year_built'].astype(int)

"""## encode categorical feature and drop useless features"""

boroughs = np.array(md_df['borough'].value_counts().index)

for borough in boroughs:
    md_df[borough] = md_df['borough'].apply(lambda x : int(x == borough))

features_notNeed = ['addr_unit', 'building_id', 'created_at', 'addr_street', 'addr_city', 'addr_zip', 'bin', 'bbl', 'description', \
                    'neighborhood', 'unit', 'borough', 'line']

md_df = md_df.drop(features_notNeed, axis=1)
md_df.head(10).T


